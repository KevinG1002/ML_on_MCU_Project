{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kNnT0e9hWPNE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import re\n",
    "from hashlib import sha1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V8x2P3mCM3yS"
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"\n",
    "  Helper function when downloading dataset that determines which data partition the file should belong to.\n",
    "\n",
    "  We want to keep files in the same training, validation, or testing sets even\n",
    "  if new ones are added over time. This makes it less likely that testing\n",
    "  samples will accidentally be reused in training when long runs are restarted\n",
    "  for example. To keep this stability, a hash of the filename is taken and used\n",
    "  to determine which set it should belong to. This determination only depends on\n",
    "  the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "  It's also useful to associate particular files as related (for example words\n",
    "  spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "  ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "  'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "  Args:\n",
    "    filename: File path of the data sample.\n",
    "    validation_percentage: How much of the data set to use for validation.\n",
    "    testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "  Returns:\n",
    "    String, one of 'training', 'validation', or 'testing'.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "    base_name = os.path.basename(filename)\n",
    "\n",
    "      # We want to ignore anything after '_nohash_' in the file name when\n",
    "      # deciding which set to put a wav in, so the data set creator has a way of\n",
    "      # grouping wavs that are close variations of each other.\n",
    "\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode('utf-8')\n",
    "\n",
    "      # This looks a bit magical, but we need to decide whether this file should\n",
    "      # go into the training, testing, or validation sets, and we want to keep\n",
    "      # existing files in the same set even if more files are subsequently\n",
    "      # added.\n",
    "      # To do that, we need a stable way of deciding based on just the file name\n",
    "      # itself, so we do a hash of that and then use that to generate a\n",
    "      # probability value that we use to assign it.\n",
    "\n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "    percentage_hash = int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS\n",
    "            + 1) * (100.0 / MAX_NUM_WAVS_PER_CLASS)\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < testing_percentage + validation_percentage:\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataset_list):\n",
    "    \"\"\"\n",
    "Samples are the files themselves upon which we will apply a feature extraction function.\n",
    "Labels are the parent directories of the file. Thus, this function returns the label of a given wav file. \n",
    "Function takes as input either a text file containing wav file paths OR a list of wav file paths.  \n",
    "    \"\"\"\n",
    "    training_labels = []\n",
    "    if type(dataset_list) == list:\n",
    "        for file in dataset_list:\n",
    "            training_labels.append(os.path.dirname(file))\n",
    "        return np.asarray(training_labels)\n",
    "    \n",
    "    elif os.path.isfile(dataset_list):\n",
    "        with open(dataset_list) as f:\n",
    "            dataset_files = f.readlines()\n",
    "            for file in dataset_files:\n",
    "                training_labels.append(os.path.dirname(file))\n",
    "        return np.asarray(training_labels)\n",
    "    else:\n",
    "        return \"Sorry, wrong input format provided\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nB0i4w6v6Shn"
   },
   "outputs": [],
   "source": [
    "def create_training_txt_file(dataset_path):\n",
    "    \n",
    "    with open(os.path.join(dataset_path, 'testing_list.txt')) as f:\n",
    "        testing_files = f.read().splitlines()\n",
    "\n",
    "\n",
    "    with open(os.path.join(dataset_path, 'validation_list.txt')) as f:\n",
    "        validation_files = f.read().splitlines()\n",
    "        \n",
    "    complete_dataset = []\n",
    "    with open(os.path.join(dataset_path, 'training_list.txt'), \"w\") as train_f:\n",
    "        list_of_local_dirs = [folder for folder in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, folder))]\n",
    "        for folder in list_of_local_dirs:\n",
    "            for file in os.listdir(folder_path):\n",
    "                if which_set(os.path.join(folder,file), 10, 10) == \"training\":\n",
    "                    train_f.write(f\"{os.path.join(folder, file)}\\n\")\n",
    "        \n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_data(training_list_path, training_labels_encoded, testing_list_path, testing_labels_encoded, validation_list_path, validation_labels_encoded):\n",
    "    \n",
    "    size_of_sample_considered = 1 #length of the audio file in seconds\n",
    "    fs = 16000 #the stipulated sampling rate according to the dataset description \n",
    "    segmentLength = 1024 # no. of samples to use per segment window \n",
    "    adjusted_sample_length = int(size_of_sample_considered*fs/segmentLength)*segmentLength # size of audio sample adjusted to be a power of 2. \n",
    "\n",
    "    with open(training_list_path) as f1:\n",
    "        X_training_audio_dataset_paths = f1.read().splitlines()\n",
    "        \n",
    "    with open(testing_list_path) as f2:\n",
    "        X_testing_audio_dataset_paths = f2.read().splitlines()\n",
    "        \n",
    "    with open(validation_list_path) as f3:\n",
    "        X_validation_audio_dataset_paths = f3.read().splitlines()\n",
    "    \n",
    "    \n",
    "    def get_stratified_sample(audio_file_paths, audio_file_labels, fraction):\n",
    "        X_stratified, _, y_stratified, _ = train_test_split(audio_file_paths,audio_file_labels, train_size = fraction, stratify = audio_file_labels)\n",
    "        return X_stratified, y_stratified\n",
    "        \n",
    "    Training_size = 80000\n",
    "    fraction_of_original_size = float(Training_size)/len(X_training_audio_dataset_paths)\n",
    "    \n",
    "    X_training_audio_dataset_paths, y_train = get_stratified_sample(X_training_audio_dataset_paths, training_labels_encoded, fraction_of_original_size)\n",
    "    \n",
    "    y_test = testing_labels_encoded \n",
    "    y_val = validation_labels_encoded\n",
    "    \n",
    "    X_train_wav = []\n",
    "    X_test_wav = []\n",
    "    X_val_wav = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in tqdm(range(Training_size)):\n",
    "        try: \n",
    "            fs, train_sample_wav = wavfile.read(os.path.join(dataset_path, X_training_audio_dataset_paths[i]))\n",
    "        except ValueError:\n",
    "            print(os.path.join(dataset_path, X_training_audio_dataset_paths[i]))\n",
    "            pass\n",
    "            \n",
    "        _dummy_sample_wav = train_sample_wav.copy() # get copy of wav file that you can modify\n",
    "        _dummy_sample_wav.resize(adjusted_sample_length)\n",
    "        _dummy_sample_wav = _dummy_sample_wav.reshape(-1, segmentLength)\n",
    "        X_train_wav.append(_dummy_sample_wav.astype(np.float32))\n",
    "\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(len(X_testing_audio_dataset_paths))):\n",
    "        fs, test_sample_wav = wavfile.read(os.path.join(dataset_path, X_testing_audio_dataset_paths[i]))\n",
    "        _dummy_sample_wav = test_sample_wav.copy()\n",
    "        _dummy_sample_wav.resize(adjusted_sample_length)\n",
    "        _dummy_sample_wav = _dummy_sample_wav.reshape(-1, segmentLength)\n",
    "        X_test_wav.append(_dummy_sample_wav.astype(np.float32))\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(len(X_validation_audio_dataset_paths))):\n",
    "        fs, val_sample_wav = wavfile.read(os.path.join(dataset_path, X_validation_audio_dataset_paths[i]))\n",
    "        _dummy_sample_wav = val_sample_wav.copy()\n",
    "        _dummy_sample_wav.resize(adjusted_sample_length)\n",
    "        _dummy_sample_wav = _dummy_sample_wav.reshape(-1, segmentLength)\n",
    "        X_val_wav.append(_dummy_sample_wav.astype(np.float32))\n",
    "        \n",
    "        \n",
    "    return X_train_wav, X_test_wav, X_val_wav, y_train, y_test, y_val\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfccs(samples, fs, upper_edge_hz, lower_edge_hz, num_mel_bins, num_mfcc):\n",
    "    frame_length = 1024\n",
    "    stfts = tf.signal.stft(samples, frame_length=frame_length, frame_step=frame_length, fft_length=frame_length) # no overlap\n",
    "    spectrograms = tf.abs(stfts)\n",
    "    spectrograms = tf.reshape(spectrograms, (spectrograms.shape[0],spectrograms.shape[1],-1))\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins, fs, lower_edge_hz, upper_edge_hz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :num_mfcc]\n",
    "    return tf.reshape(mfccs, (mfccs.shape[0],mfccs.shape[1],mfccs.shape[2],-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Creation of Text Files containing Samples for Training, Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global dataset_path\n",
    "dataset_path = os.path.join(\"speech_commands_v0.02\")\n",
    "training_dataset_path = os.path.join(dataset_path, \"training_list.txt\")\n",
    "testing_dataset_path = os.path.join(dataset_path, \"testing_list.txt\")\n",
    "validation_dataset_path = os.path.join(dataset_path, \"validation_list.txt\")\n",
    "if not os.path.exists(training_dataset_path):\n",
    "    create_training_txt_file(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11005\n",
      "84849\n",
      "9981\n"
     ]
    }
   ],
   "source": [
    "testing_labels = get_labels(testing_dataset_path)\n",
    "print(len(testing_labels))\n",
    "training_labels = get_labels(training_dataset_path)\n",
    "print(len(training_labels))\n",
    "validation_labels = get_labels(validation_dataset_path)\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_background_noise_' 'backward' 'bed' 'bird' 'cat' 'dog' 'down' 'eight'\n",
      " 'five' 'follow' 'forward' 'four' 'go' 'happy' 'house' 'learn' 'left'\n",
      " 'marvin' 'nine' 'no' 'off' 'on' 'one' 'right' 'seven' 'sheila' 'six'\n",
      " 'stop' 'three' 'tree' 'two' 'up' 'visual' 'wow' 'yes' 'zero']\n"
     ]
    }
   ],
   "source": [
    "training_encoder = LabelEncoder()\n",
    "training_encoder.fit(training_labels)\n",
    "training_labels_encoded = training_encoder.transform(training_labels)\n",
    "\n",
    "# training_encoder.fit(testing_labels)\n",
    "testing_labels_encoded = training_encoder.transform(testing_labels)\n",
    "\n",
    "validation_labels_encoded = training_encoder.transform(validation_labels)\n",
    "\n",
    "\n",
    "print(training_encoder.classes_)\n",
    "\n",
    "# sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Getting an idea of class counts in Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Counter({'zero': 3250, 'five': 3240, 'yes': 3228, 'seven': 3205, 'nine': 3170, 'one': 3140, 'down': 3134, 'no': 3130, 'stop': 3111, 'two': 3111, 'go': 3106, 'six': 3088, 'on': 3086, 'left': 3037, 'eight': 3033, 'right': 3019, 'off': 2970, 'three': 2966, 'four': 2955, 'up': 2948, 'house': 1727, 'wow': 1724, 'dog': 1711, 'marvin': 1710, 'bird': 1697, 'cat': 1657, 'happy': 1632, 'sheila': 1606, 'bed': 1594, 'tree': 1407, 'backward': 1346, 'visual': 1288, 'learn': 1286, 'follow': 1275, 'forward': 1256, '_background_noise_': 6})\n",
      "\n",
      "Testing: Counter({'five': 445, 'up': 425, 'two': 424, 'yes': 419, 'zero': 418, 'left': 412, 'stop': 411, 'eight': 408, 'nine': 408, 'seven': 406, 'down': 406, 'no': 405, 'three': 405, 'go': 402, 'off': 402, 'four': 400, 'one': 399, 'right': 396, 'on': 396, 'six': 394, 'dog': 220, 'sheila': 212, 'bed': 207, 'wow': 206, 'happy': 203, 'marvin': 195, 'cat': 194, 'tree': 193, 'house': 191, 'bird': 185, 'follow': 172, 'backward': 165, 'visual': 165, 'learn': 161, 'forward': 155})\n",
      "\n",
      "Validation: Counter({'no': 406, 'yes': 397, 'seven': 387, 'zero': 384, 'six': 378, 'down': 377, 'off': 373, 'four': 373, 'go': 372, 'five': 367, 'right': 363, 'on': 363, 'nine': 356, 'three': 356, 'left': 352, 'one': 351, 'stop': 350, 'up': 350, 'eight': 346, 'two': 345, 'happy': 219, 'bed': 213, 'sheila': 204, 'dog': 197, 'marvin': 195, 'house': 195, 'wow': 193, 'bird': 182, 'cat': 180, 'tree': 159, 'backward': 153, 'forward': 146, 'visual': 139, 'follow': 132, 'learn': 128})\n",
      "\n",
      " Counter({35: 3250, 8: 3240, 34: 3228, 24: 3205, 18: 3170, 22: 3140, 6: 3134, 19: 3130, 27: 3111, 30: 3111, 12: 3106, 26: 3088, 21: 3086, 16: 3037, 7: 3033, 23: 3019, 20: 2970, 28: 2966, 11: 2955, 31: 2948, 14: 1727, 33: 1724, 5: 1711, 17: 1710, 3: 1697, 4: 1657, 13: 1632, 25: 1606, 2: 1594, 29: 1407, 1: 1346, 32: 1288, 15: 1286, 9: 1275, 10: 1256, 0: 6})\n",
      "\n",
      " Counter({8: 445, 31: 425, 30: 424, 34: 419, 35: 418, 16: 412, 27: 411, 7: 408, 18: 408, 24: 406, 6: 406, 19: 405, 28: 405, 12: 402, 20: 402, 11: 400, 22: 399, 23: 396, 21: 396, 26: 394, 5: 220, 25: 212, 2: 207, 33: 206, 13: 203, 17: 195, 4: 194, 29: 193, 14: 191, 3: 185, 9: 172, 1: 165, 32: 165, 15: 161, 10: 155})\n"
     ]
    }
   ],
   "source": [
    "print(\"Training:\", Counter(training_labels))\n",
    "print(\"\\nTesting:\", Counter(testing_labels))\n",
    "print(\"\\nValidation:\", Counter(validation_labels))\n",
    "\n",
    "print(\"\\n\",Counter(training_labels_encoded))\n",
    "print(\"\\n\",Counter(testing_labels_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Processing Pipeline on WAV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit about the audio files at hand:\n",
    "- We have a total of 105 829 audio files split in the following way for Training, Testing and Validation:\n",
    "    - Training: 84850\n",
    "    - Testing: 11005\n",
    "    - Validation: 9981\n",
    "- Each audio file was sampled at a 16000 Hz rate and each file is *trimmed down to one second length*. Thus per audio file, you can expect 16000 samples of information describing that file.\n",
    "- The breakdown of classes found in each set is as follows:\n",
    "    - Training Set: '_background_noise_','backward','bed','bird','cat','dog','down','eight','five','follow','forward','four','go','happy','house','learn','left','marvin','nine','no','off','on','one','right','seven','sheila','six','stop','three','tree','two' 'up','visual','wow','yes','zero'.\n",
    "    - Testing Set: 'backward','bed','bird','cat','dog','down','eight','five','follow','forward','four','go','happy','house','learn','left','marvin','nine','no','off','on','one','right','seven','sheila','six','stop','three','tree','two' 'up','visual','wow','yes','zero'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 22655/80000 [00:33<01:38, 580.17it/s]/usr/local/lib/python3.7/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n",
      "100%|██████████| 80000/80000 [02:05<00:00, 634.93it/s]\n",
      "100%|██████████| 11005/11005 [00:15<00:00, 714.37it/s]\n",
      "100%|██████████| 9981/9981 [00:16<00:00, 621.31it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = load_audio_data(training_dataset_path, training_labels_encoded, testing_dataset_path, testing_labels_encoded, validation_dataset_path, validation_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC extraction time: 356.79206132888794 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train_mfccs = compute_mfccs(X_train, fs = 16000, upper_edge_hz = 8000.0, lower_edge_hz = 60.0, num_mel_bins = 80, num_mfcc = 13)\n",
    "end = time.time()\n",
    "print(\"MFCC extraction time:\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC extraction time: 28.364768028259277 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_test_mfccs = compute_mfccs(X_test, fs = 16000, upper_edge_hz = 8000.0, lower_edge_hz = 60.0, num_mel_bins = 80, num_mfcc = 13)\n",
    "end = time.time()\n",
    "print(\"MFCC extraction time:\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC extraction time: 26.400229930877686 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_val_mfccs = compute_mfccs(X_val, fs = 16000, upper_edge_hz = 8000.0, lower_edge_hz = 60.0, num_mel_bins = 80, num_mfcc = 13)\n",
    "end = time.time()\n",
    "print(\"MFCC extraction time:\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (80000, 15, 13, 1)\n",
      "X_test shape: (11005, 15, 13, 1)\n",
      "X_val shape: (9981, 15, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train_mfccs.shape)\n",
    "print(\"X_test shape:\", X_test_mfccs.shape)\n",
    "print(\"X_val shape:\", X_val_mfccs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation of Conv Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reasons we don't normalize to mean 0 and variance = 1:\n",
    "The performance of CMVN is known to degrade for short utterances due to insufficient data for \n",
    "parameter estimation and loss of discriminable information as all utterances are forced to \n",
    "have zero mean and unit variance. This affects us as we're dealing with 1 second samples for \n",
    "training.\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "\n",
    "train_set = X_train_mfccs\n",
    "train_labels = y_train\n",
    "\n",
    "test_set = X_test_mfccs\n",
    "test_labels = y_test\n",
    "\n",
    "val_set = (X_val_mfccs)\n",
    "val_labels = y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, epochs, train_set, train_labels, val_set, val_labels):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    #model.add(layers.InputLayer(input_shape=(train_set.shape[1],train_set.shape[2],train_set.shape[3]), batch_size= batch_size))\n",
    "    model.add(layers.Conv2D(filters=3,kernel_size=(5,5),padding=\"same\",input_shape=(train_set[0].shape)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=16,kernel_size=(3,3),padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "\n",
    "    model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=32,kernel_size=(3,3),padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "\n",
    "    model.add(layers.MaxPool2D((4,4)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=48,kernel_size=(3,3),padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(8))\n",
    "    model.add(layers.Activation('relu'))\n",
    "\n",
    "    model.add(layers.Dense(36))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    model.fit(train_set, train_labels, batch_size, epochs, validation_data=(val_set, val_labels))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "def quantization_aware_training(model):\n",
    "    q_aware_model = tfmot.quantization.keras.quantize_model(model) # q_aware stands for for quantization aware.\n",
    "    q_aware_model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy']) # `quantize_model` requires a recompile.\n",
    "    return q_aware_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 9981 samples\n",
      "80000/80000 [==============================] - 45s 568us/sample - loss: 2.1357 - accuracy: 0.3859 - val_loss: 1.6720 - val_accuracy: 0.5075\n",
      "Train on 80000 samples, validate on 9981 samples\n",
      "Epoch 1/30\n",
      "80000/80000 [==============================] - 45s 562us/sample - loss: 2.1934 - accuracy: 0.3746 - val_loss: 2.8770 - val_accuracy: 0.3044\n",
      "Epoch 2/30\n",
      "80000/80000 [==============================] - 44s 553us/sample - loss: 1.4809 - accuracy: 0.5646 - val_loss: 2.0717 - val_accuracy: 0.4287\n",
      "Epoch 3/30\n",
      "80000/80000 [==============================] - 44s 553us/sample - loss: 1.3156 - accuracy: 0.6124 - val_loss: 2.0288 - val_accuracy: 0.4481\n",
      "Epoch 4/30\n",
      "80000/80000 [==============================] - 45s 565us/sample - loss: 1.2391 - accuracy: 0.6341 - val_loss: 1.4819 - val_accuracy: 0.5586\n",
      "Epoch 5/30\n",
      "80000/80000 [==============================] - 45s 559us/sample - loss: 1.1943 - accuracy: 0.6468 - val_loss: 1.3502 - val_accuracy: 0.6078\n",
      "Epoch 6/30\n",
      "80000/80000 [==============================] - 45s 564us/sample - loss: 1.1530 - accuracy: 0.6604 - val_loss: 1.1444 - val_accuracy: 0.6622\n",
      "Epoch 7/30\n",
      "80000/80000 [==============================] - 42s 527us/sample - loss: 1.1311 - accuracy: 0.6684 - val_loss: 1.1993 - val_accuracy: 0.6439\n",
      "Epoch 8/30\n",
      "80000/80000 [==============================] - 43s 541us/sample - loss: 1.1082 - accuracy: 0.6727 - val_loss: 1.1195 - val_accuracy: 0.6725\n",
      "Epoch 9/30\n",
      "80000/80000 [==============================] - 44s 544us/sample - loss: 1.0962 - accuracy: 0.6776 - val_loss: 1.1868 - val_accuracy: 0.6534\n",
      "Epoch 10/30\n",
      "80000/80000 [==============================] - 44s 552us/sample - loss: 1.0806 - accuracy: 0.6822 - val_loss: 1.0757 - val_accuracy: 0.6774\n",
      "Epoch 11/30\n",
      "80000/80000 [==============================] - 44s 550us/sample - loss: 1.0638 - accuracy: 0.6858 - val_loss: 1.2519 - val_accuracy: 0.6441\n",
      "Epoch 12/30\n",
      "80000/80000 [==============================] - 44s 545us/sample - loss: 1.0539 - accuracy: 0.6908 - val_loss: 0.9648 - val_accuracy: 0.7166\n",
      "Epoch 13/30\n",
      "80000/80000 [==============================] - 43s 541us/sample - loss: 1.0456 - accuracy: 0.6919 - val_loss: 1.4939 - val_accuracy: 0.5743\n",
      "Epoch 14/30\n",
      "80000/80000 [==============================] - 44s 547us/sample - loss: 1.0344 - accuracy: 0.6943 - val_loss: 1.1995 - val_accuracy: 0.6499\n",
      "Epoch 15/30\n",
      "80000/80000 [==============================] - 43s 532us/sample - loss: 1.0237 - accuracy: 0.6977 - val_loss: 1.1767 - val_accuracy: 0.6574\n",
      "Epoch 16/30\n",
      "80000/80000 [==============================] - 42s 524us/sample - loss: 1.0179 - accuracy: 0.7005 - val_loss: 1.0368 - val_accuracy: 0.7026\n",
      "Epoch 17/30\n",
      "80000/80000 [==============================] - 42s 528us/sample - loss: 1.0139 - accuracy: 0.7023 - val_loss: 0.9894 - val_accuracy: 0.7026\n",
      "Epoch 18/30\n",
      "80000/80000 [==============================] - 46s 577us/sample - loss: 1.0095 - accuracy: 0.7024 - val_loss: 1.2011 - val_accuracy: 0.6396\n",
      "Epoch 19/30\n",
      "80000/80000 [==============================] - 44s 548us/sample - loss: 0.9988 - accuracy: 0.7062 - val_loss: 1.2154 - val_accuracy: 0.6287\n",
      "Epoch 20/30\n",
      "80000/80000 [==============================] - 46s 578us/sample - loss: 0.9917 - accuracy: 0.7089 - val_loss: 1.0014 - val_accuracy: 0.7040\n",
      "Epoch 21/30\n",
      "80000/80000 [==============================] - 46s 581us/sample - loss: 0.9892 - accuracy: 0.7081 - val_loss: 0.9735 - val_accuracy: 0.7075\n",
      "Epoch 22/30\n",
      "80000/80000 [==============================] - 46s 576us/sample - loss: 0.9855 - accuracy: 0.7105 - val_loss: 0.9823 - val_accuracy: 0.7090\n",
      "Epoch 23/30\n",
      "80000/80000 [==============================] - 47s 584us/sample - loss: 0.9833 - accuracy: 0.7106 - val_loss: 1.0785 - val_accuracy: 0.6823\n",
      "Epoch 24/30\n",
      "80000/80000 [==============================] - 44s 553us/sample - loss: 0.9787 - accuracy: 0.7126 - val_loss: 1.0030 - val_accuracy: 0.7042\n",
      "Epoch 25/30\n",
      "80000/80000 [==============================] - 46s 574us/sample - loss: 0.9731 - accuracy: 0.7132 - val_loss: 0.9431 - val_accuracy: 0.7201\n",
      "Epoch 26/30\n",
      "80000/80000 [==============================] - 46s 574us/sample - loss: 0.9731 - accuracy: 0.7128 - val_loss: 0.8958 - val_accuracy: 0.7356\n",
      "Epoch 27/30\n",
      "80000/80000 [==============================] - 45s 567us/sample - loss: 0.9677 - accuracy: 0.7151 - val_loss: 0.9986 - val_accuracy: 0.6998\n",
      "Epoch 28/30\n",
      "80000/80000 [==============================] - 44s 550us/sample - loss: 0.9648 - accuracy: 0.7163 - val_loss: 1.1102 - val_accuracy: 0.6809\n",
      "Epoch 29/30\n",
      "80000/80000 [==============================] - 45s 565us/sample - loss: 0.9629 - accuracy: 0.7172 - val_loss: 0.9988 - val_accuracy: 0.7088\n",
      "Epoch 30/30\n",
      "80000/80000 [==============================] - 45s 564us/sample - loss: 0.9599 - accuracy: 0.7181 - val_loss: 1.0219 - val_accuracy: 0.6966\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Prepared for Quantization\")\n",
    "prepared_model = train(batch_size, 1, train_set, train_labels, val_set, val_labels)\n",
    "print(\"\\nOriginal Model trained without Quantization\")\n",
    "og_model = train(batch_size, 30, train_set, train_labels, val_set, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 9981 samples\n",
      "Epoch 1/30\n",
      "80000/80000 [==============================] - 60s 755us/sample - loss: 1.4788 - accuracy: 0.5686 - val_loss: 1.4064 - val_accuracy: 0.5777\n",
      "Epoch 2/30\n",
      "52512/80000 [==================>...........] - ETA: 19s - loss: 1.3079 - accuracy: 0.6187"
     ]
    }
   ],
   "source": [
    "q_aware_model = quantization_aware_training(prepared_model)\n",
    "q_aware_model.fit(train_set, train_labels, batch_size, 30, validation_data=(val_set, val_labels))\n",
    "\n",
    "og_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_score = og_model.evaluate(test_set, test_labels, batch_size = 16, verbose = 0)\n",
    "q_aware_score = q_aware_model.evaluate(test_set, test_labels, batch_size = 16, verbose = 0)\n",
    "print(\"OG Trained Model Accuracy:\", original_score[1]*100)\n",
    "print(\"Quant-Aware Trained Model Accuracy:\",q_aware_score[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_model.save(\"First_it.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "#quantized_tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of Model from .h file to TFLite file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the helper functions from the Week 6 lab dealing with MFCC coefficients, we complete the conversion process as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert tensor types to numpy types prior to conversion\n",
    "\"\"\"\n",
    "def check_correct_type(input_set):\n",
    "    assert type(input_set) == np.ndarray, f\"Wrong type. Your set is of type {type(input_set)} when it should be np.ndarray\"\n",
    "check_correct_type(train_labels)\n",
    "check_correct_type(train_set)\n",
    "check_correct_type(test_labels)\n",
    "check_correct_type(test_set)\n",
    "\n",
    "\n",
    "#train_set = train_set.numpy()\n",
    "#test_set = test_set.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"First_it\" #name given to h5 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_converter(model, train_set: np.ndarray, representative_dataset_size: int, model_name: str):\n",
    "    \"\"\"\n",
    "Model conversion function derived from Week 6 lab to convert Keras model to TFLite, which\n",
    "quantizes the weights of our model using 8-bit integer quantization.\n",
    "\n",
    "Parameters:\n",
    "    model: trained Keras model contained in .h5 file\n",
    "    training_set: training set used to train Keras model\n",
    "    representative_dataset_size: Integer value representing size of subset of dataset used to train \n",
    "        our model. It should only be a value around a few hundred as it only serves \"to calibrate or estimate the range, \n",
    "        of floating-point arrays in the model (such as model input, activation outputs of intermediate layers, and model output) \n",
    "        for quantization.\" (TFLite Documentation)\n",
    "    model_name: string value denoting the name of the file on which the Keras Model is persisted.\n",
    "    \n",
    "Outputs:\n",
    "    No output as a result of this function call. New file created and persisted containing quantized\n",
    "    model. Has '.tflite' extension.\n",
    "\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model) # Convert the model to the TensorFlow Lite format with quantization\n",
    "    quantize = True\n",
    "    if (quantize):\n",
    "        def representative_dataset():\n",
    "            for i in range(representative_dataset_size):\n",
    "                yield([train_set[i].reshape(1,15,13,1)])\n",
    "        # Set the optimization flag.\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#         Enforce full-int8 quantization\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int16  # or tf.uint8\n",
    "        converter.inference_output_type = tf.int16  # or tf.uint8\n",
    "        Provide a representative dataset to ensure we quantize correctly.\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    open(model_name + '.tflite', 'wb').write(tflite_model)\n",
    "    return tflite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_c_array(quantized_model, model_name):\n",
    "    \"\"\"\n",
    "Lab Week 6 Function: Convert some hex value into an array for C programming.\n",
    "Then write TFLite model to a C source (or header) file to be used on Cube AI platform.\n",
    "\n",
    "Parameters:\n",
    "    quantized model: contains weights quantized to 8 bit fixed-point representations.\n",
    "    model_name: name of tflite model.  \n",
    "    \n",
    "    \"\"\"\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + model_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + model_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nunsigned int ' + model_name + '_len = ' + str(len(quantized_model)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'unsigned char ' + model_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(quantized_model) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(quantized_model):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "    #print(c_str)\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + model_name.upper() + '_H'\n",
    "    with open(model_name + '.h', 'w') as file:\n",
    "        file.write(c_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = model_converter(og_model, train_set, 100, model_name)\n",
    "hex_to_c_array(quantized_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def quantized_model_comparison(original_score, quantized_model, test_set, test_labels, label_encoder):\n",
    "    \"\"\"\n",
    "Function inspired from Lab Week 6 to compute quality of network after quantization. \n",
    "    \"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_content=quantized_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "#     print(\"== Input details ==\")\n",
    "#     print(\"\\n\\n\",input_details)\n",
    "#     print(\"name:\", input_details[0]['name'])\n",
    "#     print(\"shape:\", input_details[0]['shape'])\n",
    "#     print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "#     print(\"\\n== Output details ==\")\n",
    "#     print(\"name:\", output_details[0]['name'])\n",
    "#     print(\"shape:\", output_details[0]['shape'])\n",
    "#     print(\"type:\", output_details[0]['dtype'])\n",
    "    \n",
    "    \n",
    "    predictions = np.zeros((len(test_set),), dtype=int)\n",
    "#     input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "    for i in range(len(test_set)):\n",
    "        #val_batch = test_set[i]\n",
    "        test_sample = test_set[i]\n",
    "        #val_batch = val_batch / input_scale + input_zero_point\n",
    "        test_sample = np.expand_dims(test_sample, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_sample)\n",
    "        #tflite_interpreter.allocate_tensors()\n",
    "        #tflite_interpreter.invoke()\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        #tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "        #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "        #output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "        output = interpreter.tensor(output_index)\n",
    "        predictions[i] = np.argmax(output()[0])\n",
    "    test_labels_transformed = label_encoder.inverse_transform(test_labels)\n",
    "    predictions_transformed= label_encoder.inverse_transform(predictions)\n",
    "    c_matrix = consusion_matrix(test_labels_transformed, predictions_transformed, labels=None, sample_weight=None, normalize=None)    \n",
    "    sum = 0\n",
    "    for i in tqdm(range(len(predictions))):\n",
    "        if (predictions[i] == test_labels[i]):\n",
    "            sum = sum + 1\n",
    "    accuracy_score = sum / len(predictions)\n",
    "    print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))\n",
    "    print(\"Compared to float32 accuracy of {}%\".format(score[1]*100))\n",
    "    print(\"We have a change of {}%\".format((accuracy_score-score[1])*100))\n",
    "    \n",
    "    display_cm = ConfusionMatrixDisplay(c_matrix, labels = label_encoder.classes_)\n",
    "    display_cm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantized_model_comparison(score,quantized_tflite_model,test_set, test_labels)\n",
    "quantized_model_comparison(original_score,quantized_model,test_set, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML_on_MCU_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
