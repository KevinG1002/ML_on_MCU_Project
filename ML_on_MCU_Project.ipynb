{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kNnT0e9hWPNE"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a33730c5d03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwavfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import re\n",
    "from hashlib import sha1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V8x2P3mCM3yS"
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"\n",
    "  Helper function when downloading dataset that determines which data partition the file should belong to.\n",
    "\n",
    "  We want to keep files in the same training, validation, or testing sets even\n",
    "  if new ones are added over time. This makes it less likely that testing\n",
    "  samples will accidentally be reused in training when long runs are restarted\n",
    "  for example. To keep this stability, a hash of the filename is taken and used\n",
    "  to determine which set it should belong to. This determination only depends on\n",
    "  the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "  It's also useful to associate particular files as related (for example words\n",
    "  spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "  ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "  'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "  Args:\n",
    "    filename: File path of the data sample.\n",
    "    validation_percentage: How much of the data set to use for validation.\n",
    "    testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "  Returns:\n",
    "    String, one of 'training', 'validation', or 'testing'.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "    base_name = os.path.basename(filename)\n",
    "\n",
    "      # We want to ignore anything after '_nohash_' in the file name when\n",
    "      # deciding which set to put a wav in, so the data set creator has a way of\n",
    "      # grouping wavs that are close variations of each other.\n",
    "\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode('utf-8')\n",
    "\n",
    "      # This looks a bit magical, but we need to decide whether this file should\n",
    "      # go into the training, testing, or validation sets, and we want to keep\n",
    "      # existing files in the same set even if more files are subsequently\n",
    "      # added.\n",
    "      # To do that, we need a stable way of deciding based on just the file name\n",
    "      # itself, so we do a hash of that and then use that to generate a\n",
    "      # probability value that we use to assign it.\n",
    "\n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "    percentage_hash = int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS\n",
    "            + 1) * (100.0 / MAX_NUM_WAVS_PER_CLASS)\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < testing_percentage + validation_percentage:\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataset_list):\n",
    "    \"\"\"\n",
    "Samples are the files themselves upon which we will apply a feature extraction function.\n",
    "Labels are the parent directories of the file. Thus, this function returns the label of a given wav file. \n",
    "Function takes as input either a text file containing wav file paths OR a list of wav file paths.  \n",
    "    \"\"\"\n",
    "    training_labels = []\n",
    "    if type(dataset_list) == list:\n",
    "        for file in dataset_list:\n",
    "            training_labels.append(os.path.dirname(file))\n",
    "        return np.asarray(training_labels)\n",
    "    \n",
    "    elif os.path.isfile(dataset_list):\n",
    "        with open(dataset_list) as f:\n",
    "            dataset_files = f.readlines()\n",
    "            for file in dataset_files:\n",
    "                training_labels.append(os.path.dirname(file))\n",
    "        return np.asarray(training_labels)\n",
    "    else:\n",
    "        return \"Sorry, wrong input format provided\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nB0i4w6v6Shn"
   },
   "outputs": [],
   "source": [
    "def create_training_txt_file(dataset_path):\n",
    "    \n",
    "    with open(os.path.join(dataset_path, 'testing_list.txt')) as f:\n",
    "        testing_files = f.read().splitlines()\n",
    "\n",
    "\n",
    "    with open(os.path.join(dataset_path, 'validation_list.txt')) as f:\n",
    "        validation_files = f.read().splitlines()\n",
    "        \n",
    "    complete_dataset = []\n",
    "    with open(os.path.join(dataset_path, 'training_list.txt'), \"w\") as train_f:\n",
    "        list_of_local_dirs = [folder for folder in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, folder))]\n",
    "        for folder in list_of_local_dirs:\n",
    "            for file in os.listdir(folder_path):\n",
    "                if which_set(os.path.join(folder,file), 10, 10) == \"training\":\n",
    "                    train_f.write(f\"{os.path.join(folder, file)}\\n\")\n",
    "        \n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_data(training_list_path, training_labels_encoded, testing_list_path, testing_labels_encoded, validation_list_path, validation_labels_encoded):\n",
    "    \n",
    "    size_of_sample_considered = 1 #length of the audio file in seconds\n",
    "    fs = 16000 #the stipulated sampling rate according to the dataset description \n",
    "    segmentLength = 1024 # no. of samples to use per segment window \n",
    "    adjusted_sample_length = int(size_of_sample_considered*fs/segmentLength)*segmentLength # size of audio sample adjusted to be a power of 2. \n",
    "\n",
    "    with open(training_list_path) as f1:\n",
    "        X_training_audio_dataset_paths = f1.read().splitlines()\n",
    "        \n",
    "    with open(testing_list_path) as f2:\n",
    "        X_testing_audio_dataset_paths = f2.read().splitlines()\n",
    "        \n",
    "    with open(validation_list_path) as f3:\n",
    "        X_validation_audio_dataset_paths = f3.read().splitlines()\n",
    "    \n",
    "    \n",
    "    def get_stratified_sample(audio_file_paths, audio_file_labels, fraction):\n",
    "        X_stratified, _, y_stratified, _ = train_test_split(audio_file_paths,audio_file_labels, train_size = fraction, stratify = audio_file_labels)\n",
    "        return X_stratified, y_stratified\n",
    "        \n",
    "    Training_size = 80000\n",
    "    fraction_of_original_size = float(Training_size)/len(X_training_audio_dataset_paths)\n",
    "    \n",
    "    X_training_audio_dataset_paths, y_train = get_stratified_sample(X_training_audio_dataset_paths, training_labels_encoded, fraction_of_original_size)\n",
    "    \n",
    "    y_test = testing_labels_encoded \n",
    "    y_val = validation_labels_encoded\n",
    "    \n",
    "    X_train_wav = []\n",
    "    X_test_wav = []\n",
    "    X_val_wav = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in tqdm(range(Training_size)):\n",
    "        try: \n",
    "            fs, train_sample_wav = wavfile.read(os.path.join(dataset_path, X_training_audio_dataset_paths[i]))\n",
    "        except ValueError:\n",
    "            print(os.path.join(dataset_path, X_training_audio_dataset_paths[i]))\n",
    "            pass\n",
    "            \n",
    "        _dummy_sample_wav = train_sample_wav.copy() # get copy of wav file that you can modify\n",
    "        _dummy_sample_wav.resize(adjusted_sample_length)\n",
    "        _dummy_sample_wav = _dummy_sample_wav.reshape(-1, segmentLength)\n",
    "        X_train_wav.append(_dummy_sample_wav.astype(np.float32))\n",
    "\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(len(X_testing_audio_dataset_paths))):\n",
    "        fs, test_sample_wav = wavfile.read(os.path.join(dataset_path, X_testing_audio_dataset_paths[i]))\n",
    "        _dummy_sample_wav = test_sample_wav.copy()\n",
    "        _dummy_sample_wav.resize(adjusted_sample_length)\n",
    "        _dummy_sample_wav = _dummy_sample_wav.reshape(-1, segmentLength)\n",
    "        X_test_wav.append(_dummy_sample_wav.astype(np.float32))\n",
    "        \n",
    "        \n",
    "    for i in tqdm(range(len(X_validation_audio_dataset_paths))):\n",
    "        fs, val_sample_wav = wavfile.read(os.path.join(dataset_path, X_testing_audio_dataset_paths[i]))\n",
    "        _dummy_sample_wav = val_sample_wav.copy()\n",
    "        _dummy_sample_wav.resize(adjusted_sample_length)\n",
    "        _dummy_sample_wav = _dummy_sample_wav.reshape(-1, segmentLength)\n",
    "        X_val_wav.append(_dummy_sample_wav.astype(np.float32))\n",
    "        \n",
    "        \n",
    "    return X_train_wav, X_test_wav, X_val_wav, y_train, y_test, y_val\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfccs(samples, fs, upper_edge_hz, lower_edge_hz, num_mel_bins, num_mfcc):\n",
    "    frame_length = 1024\n",
    "    stfts = tf.signal.stft(samples, frame_length=frame_length, frame_step=frame_length, fft_length=frame_length) # no overlap\n",
    "    spectrograms = tf.abs(stfts)\n",
    "    spectrograms = tf.reshape(spectrograms, (spectrograms.shape[0],spectrograms.shape[1],-1))\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins, fs, lower_edge_hz, upper_edge_hz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :num_mfcc]\n",
    "    return tf.reshape(mfccs, (mfccs.shape[0],mfccs.shape[1],mfccs.shape[2],-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Creation of Text Files containing Samples for Training, Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c8d292789542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"speech_commands_v0.02\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraining_dataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_list.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtesting_dataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"testing_list.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_dataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_list.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "global dataset_path\n",
    "dataset_path = os.path.join(\"speech_commands_v0.02\")\n",
    "training_dataset_path = os.path.join(dataset_path, \"training_list.txt\")\n",
    "testing_dataset_path = os.path.join(dataset_path, \"testing_list.txt\")\n",
    "validation_dataset_path = os.path.join(dataset_path, \"validation_list.txt\")\n",
    "if not os.path.exists(training_dataset_path):\n",
    "    create_training_txt_file(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testing_dataset_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8bdd0727ae42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtesting_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtraining_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testing_dataset_path' is not defined"
     ]
    }
   ],
   "source": [
    "testing_labels = get_labels(testing_dataset_path)\n",
    "print(len(testing_labels))\n",
    "training_labels = get_labels(training_dataset_path)\n",
    "print(len(training_labels))\n",
    "validation_labels = get_labels(validation_dataset_path)\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-734918ebf9ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtraining_labels_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# training_encoder.fit(testing_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "training_encoder = LabelEncoder()\n",
    "training_encoder.fit(training_labels)\n",
    "training_labels_encoded = training_encoder.transform(training_labels)\n",
    "\n",
    "# training_encoder.fit(testing_labels)\n",
    "testing_labels_encoded = training_encoder.transform(testing_labels)\n",
    "\n",
    "validation_labels_encoded = training_encoder.transform(validation_labels)\n",
    "\n",
    "\n",
    "print(training_encoder.classes_)\n",
    "\n",
    "# sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Getting an idea of class counts in Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-07f4ced29e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTesting:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nValidation:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_labels_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Training:\", Counter(training_labels))\n",
    "print(\"\\nTesting:\", Counter(testing_labels))\n",
    "print(\"\\nValidation:\", Counter(validation_labels))\n",
    "\n",
    "print(\"\\n\",Counter(training_labels_encoded))\n",
    "print(\"\\n\",Counter(testing_labels_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Processing Pipeline on WAV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit about the audio files at hand:\n",
    "- We have a total of 105 829 audio files split in the following way for Training, Testing and Validation:\n",
    "    - Training: 84850\n",
    "    - Testing: 11005\n",
    "    - Validation: 9981\n",
    "- Each audio file was sampled at a 16000 Hz rate and each file is *trimmed down to one second length*. Thus per audio file, you can expect 16000 samples of information describing that file.\n",
    "- The breakdown of classes found in each set is as follows:\n",
    "    - Training Set: '_background_noise_','backward','bed','bird','cat','dog','down','eight','five','follow','forward','four','go','happy','house','learn','left','marvin','nine','no','off','on','one','right','seven','sheila','six','stop','three','tree','two' 'up','visual','wow','yes','zero'.\n",
    "    - Testing Set: 'backward','bed','bird','cat','dog','down','eight','five','follow','forward','four','go','happy','house','learn','left','marvin','nine','no','off','on','one','right','seven','sheila','six','stop','three','tree','two' 'up','visual','wow','yes','zero'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_dataset_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d5be26175658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_labels_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_dataset_path' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = load_audio_data(training_dataset_path, training_labels_encoded, testing_dataset_path, testing_labels_encoded, validation_dataset_path, validation_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-07818d86593d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_mfccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mfccs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_edge_hz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8000.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_edge_hz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mel_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mfcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MFCC extraction time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train_mfccs = compute_mfccs(X_train, fs = 16000, upper_edge_hz = 8000.0, lower_edge_hz = 800.0, num_mel_bins = 80, num_mfcc = 13)\n",
    "end = time.time()\n",
    "print(\"MFCC extraction time:\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-abd5c1c5218c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test_mfccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mfccs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_edge_hz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8000.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_edge_hz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mel_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mfcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MFCC extraction time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_test_mfccs = compute_mfccs(X_test, fs = 16000, upper_edge_hz = 8000.0, lower_edge_hz = 800.0, num_mel_bins = 80, num_mfcc = 13)\n",
    "end = time.time()\n",
    "print(\"MFCC extraction time:\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC extraction time: 28.3651282787323 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_val_mfccs = compute_mfccs(X_val, fs = 16000, upper_edge_hz = 8000.0, lower_edge_hz = 800.0, num_mel_bins = 80, num_mfcc = 13)\n",
    "end = time.time()\n",
    "print(\"MFCC extraction time:\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (80000, 15, 13, 1)\n",
      "X_test shape: (11005, 15, 13, 1)\n",
      "X_val shape: (9981, 15, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train_mfccs.shape)\n",
    "print(\"X_test shape:\", X_test_mfccs.shape)\n",
    "print(\"X_val shape:\", X_val_mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiation of Conv Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "train_set = (X_train_mfccs) # normalize to mean 0.5 and variance = 0\n",
    "train_labels = y_train\n",
    "print(len(train_set), len(train_labels))\n",
    "\n",
    "\n",
    "test_set = (X_test_mfccs) # normalize to mean 0.5 and variance = 0\n",
    "test_labels = y_test\n",
    "\n",
    "val_set = (X_val_mfccs)\n",
    "val_labels = y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2599d6f085da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(layers.InputLayer(input_shape=(train_set.shape[1],train_set.shape[2],train_set.shape[3]), batch_size= batch_size))\n",
    "model.add(layers.Conv2D(filters=5,kernel_size=(5,5),padding=\"same\",input_shape=(train_set[0].shape)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Conv2D(filters=16,kernel_size=(5,5),padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32,kernel_size=(5,5),padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPool2D((5,5)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=48,kernel_size=(5,5),padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(8))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(36))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
    "model.fit(train_set, train_labels, batch_size, epochs, validation_data=(val_set, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "score = model.evaluate(test_set, test_labels, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"First_it.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML_on_MCU_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
